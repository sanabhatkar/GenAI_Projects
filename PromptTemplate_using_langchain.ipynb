{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1446e5b",
   "metadata": {},
   "source": [
    "## Simple PromptTemplate examples using langchain framework\n",
    "- Examples with the different style of prompts in the context window\n",
    "- Examples showing how to control the tone of the output or completion\n",
    "- I have used ChatGPT's gpt-3.5-turbo model to perform the completions\n",
    "- I have used ChatPromptTemplate to get the output as JSON and used a output parser to format the response we recieve in a JSON type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be529916-30d9-43e1-92c4-88a202bed961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6693cec9-1177-4027-828a-8fd00d83640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b15f5c6-83c6-4b00-9d29-cb03a3fa048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b6bf8e",
   "metadata": {},
   "source": [
    "#### We are going to use a user-define utility class to get the secret keys from the environment variables set. Kinldy refer to UDCUtils.py file for the definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92ebe6a-ffc2-4658-ae55-683b71a77061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from UDCUtils import UDCUtils\n",
    "utils = UDCUtils()\n",
    "openai.api_key = utils.get_openai_api_key()\n",
    "#openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efbef21f-0174-4e0f-8f0c-b2e825685233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now()\n",
    "\n",
    "target_date = datetime.datetime(2024, 6, 12)\n",
    "\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5faa6c39-6d26-4e28-9cb2-b370a62d764b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c386983-96b9-4adb-903f-a38cc7fb80f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=llm_model):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\":prompt\n",
    "        }\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature = 0.0\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34297336",
   "metadata": {},
   "source": [
    "###### In above cell we have made use of temperature attribute, it is a configuration to set how precise or accurate you need the completion to be. 0 indicates nearly accurate response whereas a higher value like 1.0 indicates the output to be very creative ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab6e83c2-6260-4f2a-8357-9f3445f55332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\"This umbrella is the worst! I tried using it in a rainstorm and it got all '\n",
      " 'wet and soggy. What a ripoff! I thought umbrellas were supposed to keep you '\n",
      " 'dry, but this one just made everything worse. Do not recommend.\"')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "response = get_completion(\"Provide me with the dumbest product review for a umbrella by an american citizen\")\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b295ae3d-9fea-4a18-a805-34109fff4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "This umbrella is the worst! I tried using it in a rainstorm and it got all wet and soggy. \\\n",
    "What a ripoff! I thought umbrellas were supposed to keep you dry, \\\n",
    "but this one just made everything worse. Help me get a refund.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7757e75b",
   "metadata": {},
   "source": [
    "##### Now we are going to ask chat gpt to format the above customer email in a particular tone using the instructions in prompt as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3919c4e-c2c8-4b4b-b6d1-6dccd5eccd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_style = \"\"\"\n",
    "American style \\\n",
    "in a calm and polite tone.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dd81359-d13b-4c45-994e-71c292000a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_message = f\"\"\"\n",
    "Translate the text that is delimited by triple backticks \\\n",
    "into a style that is {response_style} \\\n",
    "text : ```{customer_email}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbf53d47-3b5c-4495-821d-78cca1f9407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translate the text that is delimited by triple backticks into a style that is \n",
      "American style in a calm and polite tone.\n",
      " text : ```\n",
      "This umbrella is the worst! I tried using it in a rainstorm and it got all wet and soggy. What a ripoff! I thought umbrellas were supposed to keep you dry, but this one just made everything worse. Help me get a refund.\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "472f729b-e6f7-4efe-94f8-128e79061341",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(prompt_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77e75f53-5e24-4f6c-b093-8a65c8908728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I would like to kindly express my disappointment with this umbrella. I used '\n",
      " 'it during a rainstorm and unfortunately, it became wet and soggy. I was '\n",
      " 'under the impression that umbrellas were meant to keep you dry, but this one '\n",
      " 'did not meet my expectations. I would greatly appreciate your assistance in '\n",
      " 'obtaining a refund. Thank you.')\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b52f5-eef9-481d-b55f-6322a12b0db0",
   "metadata": {},
   "source": [
    "### Lets do the above work using LangChain framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb2d5951-d50e-4775-b6d2-232cdf975229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_community\n",
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0e6a2be-00d9-4331-bae9-00b138c9e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2041b167-a7b1-47a1-b157-c202ea6ef374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=0.0,\n",
    "                 model=llm_model,\n",
    "                 openai_api_key = openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f979eb0a",
   "metadata": {},
   "source": [
    "##### Note down that we are going to use prompt templates to style and format our prompts, prompt templates make it easier to give instructions when the length of the input becomes longer and harder to format using string functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47c468e5-eafc-4d0d-b4d3-553dac145af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_style = \"\"\"\\\n",
    "Translate the provided text delimited by three backticks into a style that is {style}\\\n",
    "text : ```{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd3a5e0e-b78a-43d3-97bc-092b5c03d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3b023a4-d025-4731-953d-549b5fd001fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PromptTemplate(input_variables=['style', 'text'], template='Translate the provided text delimited by three backticks into a style that is {style}text : ```{text}\\n')\n"
     ]
    }
   ],
   "source": [
    "pprint(prompt_template.messages[0].prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "845c86c9-0dce-43de-9ee6-04d5dda944ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will give us the inputs we need to provide when sending instruction to llm\n",
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41ad2109-a338-4108-a798-45a5e1995ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing the values of input variables\n",
    "customer_message = prompt_template.format_messages(text=customer_email,\n",
    "                                                  style=response_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f779dd3-9da9-4d47-ae77-2e6808af619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a6895f0-c11e-45a9-81c7-f6343823c6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Translate the provided text delimited by three backticks into a style that is \\nAmerican style in a calm and polite tone.\\ntext : ```\\nThis umbrella is the worst! I tried using it in a rainstorm and it got all wet and soggy. What a ripoff! I thought umbrellas were supposed to keep you dry, but this one just made everything worse. Help me get a refund.\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "print(customer_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fcf6c5e-cd2c-45ab-90c9-0ef321ff3272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "customer_formatted_message = chat(customer_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abdb3854-20b9-4959-ab20-fcb53d174143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I am sorry to say that I am quite disappointed with this umbrella. I used it '\n",
      " 'during a rainstorm and it ended up getting wet and soggy, which was not what '\n",
      " 'I expected. I always thought umbrellas were meant to keep you dry, but '\n",
      " 'unfortunately, this one did not live up to that expectation. I would greatly '\n",
      " 'appreciate your assistance in helping me obtain a refund for this product. '\n",
      " 'Thank you for your understanding.')\n"
     ]
    }
   ],
   "source": [
    "pprint(customer_formatted_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5c7b4-223e-4e28-bc72-fa9167b4a883",
   "metadata": {},
   "source": [
    "#### Some fun code to get the reply as well from chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f03514b-bcd3-4e71-9ab5-a4842c5d9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_reply = \"\"\"\n",
    "You are a customer support executive. Suggest me a good reply to the customer query \\\n",
    "provided in the text delimited by three backticks and \\\n",
    "the style of the reply should be in a {style} \\\n",
    "text : ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b3f248d-cc62-436d-ba06-dd317e9cb7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(service_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02f5004d-5208-41aa-ae41-365cdaf1cf13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['style', 'text']\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.messages[0].prompt.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e23596fb-d256-4818-9eca-b1310054530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_style = \"\"\"\n",
    "Casual american style in a sarcastic tone.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24d68ee0-ef21-46fa-ad74-7240361768a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_message = prompt_template.format_messages(style=service_style,\n",
    "                                                 text=customer_formatted_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e50262c-0c86-41b0-973b-0e7d6eb21e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_service_response = chat(service_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8825ec44-8ad7-442f-a4c6-6fa10f4d5ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Well, gee, sorry to hear about your soggy umbrella experience. Who would've \"\n",
      " 'thought an umbrella would actually get wet in a rainstorm, right? I mean, '\n",
      " \"that's just unheard of! Let me see what I can do to help you out with that \"\n",
      " 'refund. Thanks for sharing your unique insight on umbrella functionality.')\n"
     ]
    }
   ],
   "source": [
    "pprint(generated_service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814fa28a-3154-45f2-a5cd-5edfc4fd085f",
   "metadata": {},
   "source": [
    "#### Well that was a funny response, right?\n",
    "#### Lets get started with output parsers in LangChain where we would be expecting completion in json format, we will read this as a JSON using a parser and can use this dict object for further operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d65c3764-1a3a-4c12-84f4-1440fbc53d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a json\n",
    "sample_json = {\n",
    "    \"gift\":True,\n",
    "    \"delivery\":\"5 days\",\n",
    "    \"price_value\":\"It is pretty affordable.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d922d383-bf11-4b77-a3cd-3096c48748c3",
   "metadata": {},
   "source": [
    "#### Again I am going to use chatgpt to generate a customer review that was purchased as a gift on birthday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "590eb7f7-510b-4447-99b0-8d4460199939",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "Suggest me a funny review about an expensive cosmetic product an American man purchased as a gift on his wife's birthday which did not make any sense.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f10a5c06-a44b-497c-9e9f-5cc959569161",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_response = get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70888cd7-5055-496d-b27f-d2a8191cd1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I bought this fancy schmancy face cream for my wife's birthday and let me tell you, it cost more than my monthly car payment. But did it make her look 10 years younger? Nope. Did it make her skin glow like a unicorn's? Not even close. All it did was make her smell like a bouquet of flowers threw up on her face. So if you're looking to waste your money on a product that makes zero sense, this is the one for you!\"\n"
     ]
    }
   ],
   "source": [
    "print(gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b98830b8-de58-477d-b616-a79101b9f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = gpt_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a75e02fe-17fc-423a-9ae4-64174d4a73c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_style = \"\"\"\n",
    "For the following text extract the below information:\n",
    "gift: Was this item purchased as a gift for someone? Answer with True if yes, else False.\n",
    "delivery_days: How many days it took for the item to get it delivered. If not known then answer with a -1.\n",
    "price_value: Extract any sentence that mentions the price or value of the item bought. \\\n",
    "If not known then answer with value as Unknown. The values for this should be a comma separated Ptyhon list.\n",
    "\n",
    "Format the output as JSON text with keys as:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "And the text is {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "573bb2ed-d972-4ada-aba4-5f5992a495d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='\\nFor the following text extract the below information:\\ngift: Was this item purchased as a gift for someone? Answer with True if yes, else False.\\ndelivery_days: How many days it took for the item to get it delivered. If not known then answer with a -1.\\nprice_value: Extract any sentence that mentions the price or value of the item bought. If not known then answer with value as Unknown. The values for this should be a comma separated Ptyhon list.\\n\\nFormat the output as JSON text with keys as:\\ngift\\ndelivery_days\\nprice_value\\n\\nAnd the text is {text}\\n'))]\n"
     ]
    }
   ],
   "source": [
    "json_promt_template = ChatPromptTemplate.from_template(response_style)\n",
    "print(json_promt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea5f28ba-d6f3-436a-b6f7-f2d758b54dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text']\n"
     ]
    }
   ],
   "source": [
    "print(json_promt_template.messages[0].prompt.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fcb08bd5-7943-48a3-8cf3-4a5fbce30144",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_product_review_prompt_message = json_promt_template.format_messages(text=customer_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d75d736-9cb6-4daf-8fe2-3d839627159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"gift\": true,\n",
      "  \"delivery_days\": -1,\n",
      "  \"price_value\": [\"it cost more than my monthly car payment.\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_formatted_output = chat(json_product_review_prompt_message)\n",
    "print(json_formatted_output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81409bb1-45be-46c9-abe8-8eeb041e8de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='{\\n  \"gift\": true,\\n  \"delivery_days\": -1,\\n  \"price_value\": [\"it cost more than my monthly car payment.\"]\\n}', response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 226, 'total_tokens': 257}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-56516a27-8697-4154-80de-7e08862aff20-0')\n"
     ]
    }
   ],
   "source": [
    "pprint(json_formatted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc4e1702-b834-45df-bb86-713c4724e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d91c2a9-16c6-4f2e-8f94-f9b220280929",
   "metadata": {},
   "outputs": [],
   "source": [
    "gift_schema = ResponseSchema(\n",
    "    name=\"gift\",\n",
    "    description=\"Was this item purchased as a gift for someone? Answer as True if yes and False if no or not known. Output should be of type boolean.\"\n",
    ")\n",
    "\n",
    "delivery_days_schema = ResponseSchema(\n",
    "    name=\"delivery_days\",\n",
    "    description=\"How many days it took for the item to get it delivered. If not known then answer with a -1.\"\n",
    ")\n",
    "\n",
    "price_val_schema = ResponseSchema(\n",
    "    name=\"price_value\",\n",
    "    description=\"Extract any sentence that mentions the price or value of the item bought. If not known then answer with value as Unknown. The values for this should be a comma separated Ptyhon list.\"\n",
    ")\n",
    "\n",
    "response_schemas = [gift_schema, delivery_days_schema, price_val_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20fd5f33-313e-4d5c-b8c5-a94ebe89b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The output should be a markdown code snippet formatted in the following '\n",
      " 'schema, including the leading and trailing \"```json\" and \"```\":\\n'\n",
      " '\\n'\n",
      " '```json\\n'\n",
      " '{\\n'\n",
      " '\\t\"gift\": string  // Was this item purchased as a gift for someone? Answer '\n",
      " 'as True if yes and False if no or not known. Output should be of type '\n",
      " 'boolean.\\n'\n",
      " '\\t\"delivery_days\": string  // How many days it took for the item to get it '\n",
      " 'delivered. If not known then answer with a -1.\\n'\n",
      " '\\t\"price_value\": string  // Extract any sentence that mentions the price or '\n",
      " 'value of the item bought. If not known then answer with value as Unknown. '\n",
      " 'The values for this should be a comma separated Ptyhon list.\\n'\n",
      " '}\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "pprint(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bc82e3f0-36a5-4b75-9ee5-8cfec8d21761",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_response_style = \"\"\"\n",
    "For the following text extract the below information:\n",
    "gift: Was this item purchased as a gift for someone? Answer as True if yes and False if no or not known. Output should be of type boolean.\n",
    "delivery_days: How many days it took for the item to get it delivered. If not known then answer with a -1.\n",
    "price_value: Extract any sentence that mentions the price or value of the item bought. \\\n",
    "If not known then answer with value as Unknown. The values for this should be a comma separated Ptyhon list.\n",
    "text:{text}\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3bb2fe6c-8ccc-4480-8436-9df3710951d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ffa4a454-a52b-4d8e-a156-0901fef88987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(parser_response_style)\n",
    "\n",
    "\n",
    "prompt_message = prompt_template.format_messages(text=customer_review,\n",
    "                                                format_instructions=format_instructions)\n",
    "\n",
    "chat_0_2 = ChatOpenAI(model=llm_model, temperature=0.2, openai_api_key = openai.api_key)\n",
    "\n",
    "json_output = chat_0_2(prompt_message)\n",
    "\n",
    "print(type(json_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ace3909b-7b24-4673-893b-f960bd87dd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": true,\n",
      "\t\"delivery_days\": -1,\n",
      "\t\"price_value\": \"it cost more than my monthly car payment\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(json_output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f171898c-77f6-4894-8e17-380434805396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(json_output.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23f05951-64f1-419f-8bbe-2bc19797974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = output_parser.parse(json_output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09a1ade1-d504-49f5-b469-c9b8a2ca4218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(output_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21a01d16-22e0-41e3-b23d-51d101ca0a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(output_dict.get(\"gift\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43170d2-c63b-432b-b729-1c1c4b6b60e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
